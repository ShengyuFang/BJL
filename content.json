{"posts":[{"title":"Raft算法论文解读与实现","text":"相关资料 论文原文 https://ramcloud.atlassian.net/wiki/download/attachments/6586375/raft.pdf Raft演示动画 https://raft.github.io/#implementations 在实现raft协议时可以参考该网站JS代码，在resources中的raft.js另个演示动画 http://thesecretlivesofdata.com/raft/ MIT6.824课程实验 https://pdos.csail.mit.edu/6.824/labs/lab-raft.html Raft基本介绍 Raft是为了解决什么问题分布式系统的一致性,分布式集群中不同节点对同一请求返回结果需要保持一致。但是，Raft一致性的前提是系统中不会有伪造的消息，因此并不能解决拜占庭将军问题。 Raft算法思路简介为了避免系统不同节点返回不同结果造成的困惑，Raft采用少数服从多数的思路，大多数(超过集群节点数一半)的节点所认为的请求响应就被当作正确的响应。偶数节点会出现一半认为A一半认为B的情况，从而导致不一致，所以Raft节点总数必须是奇数。虽然Raft是分布式算法，但是Raft采用了集中式管理的思路，集群中存在唯一LEADER节点，只有LEADER节点可以将用户请求广播给各个节点。 Raft初探 首先明确一点，一个节点发送的请求可能在网络中被延迟很久才到达目标节点，用锁之类的解决不了这种问题，什么叫线程安全得重新思考下。但是Raft算法必须要解决这一的问题才能正确运行，于是Raft采用了Term的概念，是不是很像自增的分布式主键的思想？ Raft将整个算法分为两个过程：leader选举和日志(包含了用户的命令)同步。节点有三种角色，follower candidate leader。 Raft中的role状态机 Leader选举阶段系统刚启动时所有节点都是follower，每个节点都会有个计时器，在计时器到达选举时间就转化为candidate并且让Term+1发起投票，而获得超过一半节点支持的candidate就会变成leader。多个candidate参加选举可能引起选举失败，没有节点获得超过一半的选举票这时候进入下一轮选举，Term+1。为了避免多个节点同时成为candidate发起投票，每个几点计时器都会附加随机性。 日志复制选出LEADRE节点后，LEADER节点定时发送心跳包将日志广播给其他节点。其他节点收到LEADRE心跳后重置选举时间，如果是candidate则还会重新转化为follower节点。Raft是将日志同步请求放在心跳包中进行的。 上面只是粗略介绍了Raft算法的两个过程。试想一下可能存在的问题： 怎么保证LEADER选举安全性，假设有个节点掉线很久，并且重连上后转换成了candidate发起了投票或者一个candidate的选举请求很晚才到达其他节点，这时候其他节点能给该节点投票么？如果不加限制条件的就答应投票可能会造成leader转为follower，然后不得不重新进行一轮选举，非常浪费资源。因此，这里引入raft文中第一条限制：只有candidate投票请求中的Term比收到请求的节点的term大或者相等才会把票投给它。另外为了保证一轮选举只会有一个leader产生，所以已经投过票的节点不能再投票给其他节点(默认会投自己一票，这个例外)。LEADER选举的安全性还与日志有关，这个下文章后面再讲。 日志复制给其它节点的安全性。LEADRE节点A可能掉线重连，然后A会误以为自己仍是leader(在没收到新leader的心跳包情况下)继续将同步日志发送给其它节点。或者出现这种情况，LEADER节点A发送的同步日志很晚才到其它节点，此时已经选举出新的LEADER节点B。已经选举出新leader说明至少有一半节点收到后来的candidate的投票请求，然后将TERM+1。这种情况依然是比较TERM大小，对于TERM比自己小的请求节点不会处理。而如果还有节点TERM还未+1就收到A的请求，那么它会复制A的请求，但是下一次leaderB发送心跳包时会根据日志匹配的原则将那个节点上的该日志删除掉。 总结上面的问题：什么情况下节点会同意投票请求，follower节点收到日志同步请求怎样更新自己的日志。 从上面的讨论引出raft几个关键变量：nextIndex[] matchIndex[] preIndex commitIndex。Raft原文中的applied index可以先不用理，论文中基本没说那个干嘛用的！ nextIndex[] matchIndex[] 只有leader节点需要维护这两个变量 日志同步过程 当一个节点刚成为leader节点，它不知道其他节点上的日志哪些是正确的(也就是和leader的日志是一致的)，哪些是错误的，它需要覆盖其他节点的错误日志。这里有个一直成立的条件，如果follower节点index i的日志与leader节点是一致的，那么它index i以前的日志也都会一致。这是由算法保证的。假设对某follower节点从index i开始尝试复制日志，leader节点发现自己index i的日志与follower日志不一致，那么尝试index i-1，依次类推，直到找到匹配的index。也可以采用2分法，但是有没有这个必要呢,不清楚。不清楚干脆不费劲。 为什么需要这两个变量 一个节点成为leader节点后按照2.中的办法找到匹配的index，然后对每个其他节点开启一个同步线程，将后续的日志同步给follower节点，之后来了一个用户请求后立马将其同步给其他节点就行了。这样想，似乎没有必要维护这三个变量？但是这样做有一些无法解决的问题，leader同步请求的过程中来了其他请求怎么办？可能的解决办法是加一个matchIndex[]，记录follower节点日志匹配的下标，然后不停地从matchIndex到当前日志末尾同步。那nextIndex[]干嘛用的呢？前面的”不停地”同步有问题，leader节点不停地同步太占用其他线程资源，所以raft都是通过心跳包里进行同步。每次同步时leader节点和follower节点日志都有延迟，总不能每次都将matchIndex到日志末尾的所有日志都发送给follower节点吧。所以才有了nextIndex[]，每次心跳包里包含了matchIndex到nextIndex的日志。nextIndex的另个作用也就是2.中的i,节点刚成为leader节点时nextIndex初始化leader节点日志长度(要不要减1取决代码怎么实现),设置的比较乐观。 preIndexLeader节点发送的心跳包里包含了preIndex，作用可以参考上面写的日志同步过程。 commitIndex当leader节点一个日志被超过一半的节点复制了那么改日志就被commit了，对raft本身来说commit没有用，但是调用raft的客户端需要知道哪些日志被commit了。这里有个非常大的坑，后面再提。 applied indexraft需要通知调用raft的客户端哪些日志被commit了，因此单独开一个线程去通知，每次从applied index到当前commitIndex的日志。applied index也就是上一次已经通知过的commitIndex。 Raft论文中限制条件总结Raft里的坑 matchIndex[]，论文里说matchIndex是自增的，但是怎么保证自增呢？matchIndex不自增会有的问题：leader节点刚同步完log[20:22]，然后由于matchIndex没有变成23，而又变成了20，那么它又同步了一遍，如果总是反复同步log[20:22]就跳不出循环了。matchIndex什么时候会不自增：一般情况下matchIndex总是自增的。但是如果leader发送了log[20:22]请求未被follower接收到，那么再次发送log[20:22]，这时候第一次发送的log[20:22]请求被follower接收了，于是更改matchIndex到23，然后leader发送log[23:24]，也得到响应，matchIndex改为25。这时候第二次发送的log[20:22]被follower接收了，leader获得响应，将matchIndex从25改成了23。因为程序里要特别注意别遗漏matchIndex必须自增。 commitIndex这个也是巨坑。论文没全部看完就去写代码都是泪。总结就是： Raft never commits log entries from previous terms by counting replicas。如果日志被复制到超过一半节点了不代表就可以更新commitIndex了，那个日志的term必须是当前term才行。这里就直接粘贴论文原图了。 Raft算法实现总结起来经验就是保证一个节点明明是follower却干着leader的事情，然后就是前文提到的不要把并不是多线程的问题弄成了多线程问题。从状态机那张图可以瞬间写出一个 12345678910111213141516171819for { switch rf.role { case FOLLOWER: { rf.doFollower() } case CANDIDATE: { rf.doCandidate() } case LEADER: { rf.doLeader() } default: panic(&quot;unknown role &quot; + rf.role) } }} 保证从doLeader之类函数跳出执行doFollower()时别还是跑着doLeader的线程。另外节点一边接收请求，一边发送请求，一边处理请求响应，这三者之间可能会修改了一个共享变量，引起线程安全问题，可以用锁来解决，也可以对于每个事件发送一个event到一个事件队列里，然后单线程去处理事件队列。 Raft go实现的坑 下面的代码会遗漏chan事件，别写。定时任务没执行可能是因为你写了下面这样的代码12345678910111213141516func main() { j := 0 for { select { case &lt;-time.Tick(time.Duration(10000)*time.Millisecond): { fmt.Printf(&quot;a\\n&quot;) } case &lt;-time.Tick(time.Duration(100)*time.Millisecond): { fmt.Printf(&quot;b\\n&quot;) } } } fmt.Printf(&quot;%v\\n&quot;, j)} 因为两个chan同时有事件时select随机选一个执行可以改成12345678910111213141516171819202122232425262728 func main() { j := 0 ti := time.After(time.Duration(10000)*time.Millisecond) tj := time.Tick(time.Duration(100)*time.Millisecond) for { select { case &lt;-ti: { fmt.Printf(&quot;a\\n&quot;) ti = time.After(time.Duration(10000)*time.Millisecond) } case &lt;-tj: { fmt.Printf(&quot;b\\n&quot;) tj := time.Tick(time.Duration(100)*time.Millisecond) } } } }``` 把chan拿到外面来，不要每次都返回新的chan，这样下次select时候会处理2. 对于MIT 6.824记得补全kill函数，因为实验里节点宕机重启不会把那个节点的线程全关掉，如果是leader节点，那它可能宕机了还在发送消息```gofunc (rf *Raft) Kill() { atomic.StoreInt32(&amp;rf.dead, 1) rf.stop = true} 再给个参考123456789for !rf.stop{ switch rf.role { case FOLLOWER: { rf.doFollower() } ... }}","link":"/BJL/2022/09/30/Raft%E7%AE%97%E6%B3%95%E8%AE%BA%E6%96%87%E8%A7%A3%E8%AF%BB%E4%B8%8E%E5%AE%9E%E7%8E%B0/"},{"title":"MapReduce","text":"MapReduce思想是分治算法，将原问题分解成小的子问题，最后聚合为最终结果。 MapReduce过程 MapReduce包含了master，worker结点,处理过程分为map阶段与reduce阶段。 MapReduce的worker结点会不停地向mater结点请求任务，当用户提交请求文件或SQL等请求后，master结点先将map任务返回给各个worker结点，每个map任务只处理文件/数据的某几个split分区。 当所有worker结点完成map任务表示map阶段已经结束，转到reduce阶段。这时候再请求master结点时，master结点会返回worker结点reduce任务。所有worker结点完成reduce任务后表示用户请求已经全部完成。 WordCount的MapReduceWordCount任务是统计一个段文字中各单词词。图上展示的是WordCount的MapReduce过程。MIT6.824 WordCount的输入是多个文件，简化了Split过程，与实际生产不同。实际生产输入只有一个文件，MapReduce库函数将输入数据分成多份，每份数据大小通常在16 - 64MB之间，Split过程由MapReduce自己完成。假设已经Split出10份文件，有3台Map机器，2台reduce机器。某split文件内容为”long long time ago”,由map结点2处理该文件。假设采用hash取余的方式处理key，并且hash(long)%2=0, hash(time)%2=0，hash(ago)%2=1。那么将得到文件 12M-2-0 &quot;long long time&quot;M-2-1 &quot;ago&quot; 所有split文件的map都处理完成，也就是Map阶段结束后再由reduce机器r0处理M-0-0,m-1-0,m-2-0得到输出文件MR-out-0, MR-out-0包含所有hash(单词)%2=0的单词的数量, 以此类推。 Java的MapReduce框架Java内置了单机的MapReduce库，利用多线程计算各子问题，最后返回给主线程，比如ForkJoinPool，CompletableFuture。 MapReduce思考标题起为“思考”因为下面提出的问题不一定正确。 采用MapReduce框架的平台得自己实现map与reduce函数以对使用者屏蔽，比如hive。然而感觉MR不一定适用于很多情况 MR一定需要执行完map阶段才能开始分配reduce任务吗？比如用归并算法对大文件排序采用MapReduce集群 Split的粒度问题，怎样split，分配任务到机器更能利用集群的性能？ 有哪些问题很容易改为流式计算，边读文件边处理也就是split的粒度为按行进行是不是可以提高集群利用率？待了解流式计算再与MapReduce比较。","link":"/BJL/2022/10/05/MapReduce/"},{"title":"消息队列","text":"消息队列的优点： 解耦请求与处理逻辑。 附带了持久化功能，防止请求丢失。 场景消息队列的本质还是异步处理。适用的场景有秒杀、短信邮件通知、数据统计、监控、数据同步，分布式事务，广告计费，库存扣减以及相似业务，实时性、一致性要求没那么高的业务。不适合的场景有转账（其实也用消息队列，取决于具体业务，有些银行接口需要秒级才能返回，不得不异步化），以及非必要不使用消息队列，因为消息队列会增加系统维护成本，增加延迟。不适合消息队列的案例：银行转账。转账本质是个分布式事务问题，需要保证流水和余额严格一致，所以并不适合用事务消息来解决，某大厂采用TCC方案处理事务。即使非要用事务消息来解决，也应该先在数据库事务中记录流水，再异步更新余额。这样，即使出现数据不一致的问题，也可以用流水来更正余额。反过来，没有办法通过余额反推出流水记录的。 实践步骤 Kafka,RocketMQ,RabbitMQ都不能保证在“从消息生产直到消费完成”这个过程中，消息不重不丢（Exactly once） 请求处理是否需要有序? Kafka保证了同一分区内数据有序。 请求处理防重复、幂等。导致数据处理重复的原因有：发送消息重试，处理消息重试，异步提交失败，消息队列内部重分区。幂等的实现方式有：依靠数据库主键、唯一ID、记录表、版本号、状态更新、锁等。 给消息队列开启死信队列，以便可以恢复数据。 监控消息堆积、死信队列、消息延迟等，最尴尬的是有些请求未处理并且没发现。 数据安全要求非常高再加上离线审计。实际工作里需要对齐各组数据，繁琐。","link":"/BJL/2022/10/05/%E6%B6%88%E6%81%AF%E9%98%9F%E5%88%97%E5%AE%9E%E8%B7%B5/"}],"tags":[],"categories":[{"name":"分布式","slug":"分布式","link":"/BJL/categories/%E5%88%86%E5%B8%83%E5%BC%8F/"},{"name":"工作","slug":"工作","link":"/BJL/categories/%E5%B7%A5%E4%BD%9C/"}],"pages":[{"title":"categories","text":"","link":"/BJL/categories/index.html"},{"title":"关于","text":"","link":"/BJL/about/index.html"}]}